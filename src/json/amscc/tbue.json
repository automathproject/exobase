{
  "uuid": "tbue",
  "titre": "Vraisemblance sur un modèle de translation",
  "theme": [
    "Statistique"
  ],
  "niveau": "",
  "metadata": {
    "exo7id": "",
    "auteur": "",
    "createdAt": "2025-04-04T19:05:06.710Z",
    "hasIndication": false,
    "hasCorrection": true,
    "youtube": "",
    "chapitre": "",
    "sousChapitre": "",
    "organisation": "AMSCC",
    "updatedAt": "2025-04-04T19:05:06.710Z",
    "resume": "L'exercice porte sur l'estimation du maximum de vraisemblance (EMV) pour un paramètre de translation $\\theta$. Il s'agit de calculer l'EMV de $\\theta$ étant donné un échantillon i.i.d. $X_1, ..., X_n$ dont la densité est $f_\\theta(x) = f(x - \\theta)$, où $f$ est une densité connue. L'exercice explore trois cas différents pour la densité $f$ : une loi normale $\\mathcal{N}(0, \\sigma)$ avec $\\sigma$ connu, une loi de Laplace, et une densité polynomiale uniforme sur l'intervalle [-1, 1]. L'exercice vise à appliquer la méthode du maximum de vraisemblance pour estimer un paramètre de position.",
    "competences": [
      "maximiser une fonction de vraisemblance",
      "calculer la fonction de vraisemblance pour un échantillon i.i.d.",
      "identifier et appliquer les propriétés des lois normales, de Laplace et uniforme",
      "appliquer la méthode du maximum de vraisemblance dans différents contextes",
      "interpréter les résultats de l'estimation du maximum de vraisemblance"
    ],
    "niveau_difficulte": "intermédiaire",
    "mots_cles": [
      "vraisemblance",
      "maximum de vraisemblance",
      "estimateur",
      "loi normale",
      "loi de Laplace",
      "densité de probabilité",
      "i.i.d.",
      "estimation"
    ],
    "concepts_fondamentaux": [
      "fonction de vraisemblance",
      "estimateur du maximum de vraisemblance (EMV)",
      "densité de probabilité",
      "indépendance et identité de distribution (i.i.d.)"
    ],
    "prerequis": [
      "connaissance des lois de probabilité usuelles (normale, Laplace)",
      "maîtrise du calcul différentiel (dérivation)",
      "notions de statistique descriptive",
      "compréhension de la notion de densité de probabilité"
    ],
    "type_exercice": "Exercice d'application directe",
    "temps_estime": "60 minutes"
  },
  "contenu": [
    {
      "id": "80b551c6-445e-4137-9e85-de0111e6aa92",
      "type": "description",
      "value": {
        "latex": "Soit \\( X_1, \\dots, X_n \\) i.i.d. de densité commune \\( f_\\theta(x) = f(x - \\theta) \\), \\( \\theta \\in \\mathbb{R} \\). Déterminer l'estimateur du maximum de vraisemblance dans les cas suivants :",
        "html": "<p>Soit <span class=\"math inline\">\\(X_1, \\dots, X_n\\)</span> i.i.d. de\ndensité commune <span class=\"math inline\">\\(f_\\theta(x) = f(x -\n\\theta)\\)</span>, <span class=\"math inline\">\\(\\theta \\in\n\\mathbb{R}\\)</span>. Déterminer l’estimateur du maximum de vraisemblance\ndans les cas suivants :</p>\n"
      }
    },
    {
      "id": "6369f63e-34f6-4a87-8e79-367e5862297d",
      "type": "question",
      "value": {
        "latex": "\\( f \\) est la densité de la loi \\( \\mathcal{N}(0, \\sigma^2) \\) (\\( \\sigma \\) connue) ;",
        "html": "<p><span class=\"math inline\">\\(f\\)</span> est la densité de la loi <span\nclass=\"math inline\">\\(\\mathcal{N}(0, \\sigma^2)\\)</span> (<span\nclass=\"math inline\">\\(\\sigma\\)</span> connue) ;</p>\n"
      }
    },
    {
      "id": "dafbff9d-6bd8-4262-b6bf-639e66077d58",
      "type": "reponse",
      "value": {
        "latex": "La vraisemblance est \\( L(\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(X_i - \\theta)^2}{2\\sigma^2}} \\).\n\tLa log-vraisemblance est \\( \\ell(\\theta) = \\ln L(\\theta) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i - \\theta)^2 \\).\n\tMaximiser \\( \\ell(\\theta) \\) par rapport à \\( \\theta \\) revient à minimiser la somme des carrés \\( \\sum_{i=1}^n (X_i - \\theta)^2 \\). La dérivée par rapport à \\( \\theta \\) est \\( \\frac{d}{d\\theta} \\sum_{i=1}^n (X_i - \\theta)^2 = \\sum_{i=1}^n -2(X_i - \\theta) \\). En annulant la dérivée, on obtient \\( \\sum_{i=1}^n (X_i - \\theta) = 0 \\), soit \\( \\sum X_i - n\\theta = 0 \\), d'où \\( \\theta = \\frac{1}{n} \\sum X_i = \\bar{X}_n \\).\n\t\\( \\hat{\\theta}_{MV} = \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\)",
        "html": "<p>La vraisemblance est <span class=\"math inline\">\\(L(\\theta) =\n\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(X_i -\n\\theta)^2}{2\\sigma^2}}\\)</span>. La log-vraisemblance est <span\nclass=\"math inline\">\\(\\ell(\\theta) = \\ln L(\\theta) = -\\frac{n}{2}\n\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (X_i -\n\\theta)^2\\)</span>. Maximiser <span\nclass=\"math inline\">\\(\\ell(\\theta)\\)</span> par rapport à <span\nclass=\"math inline\">\\(\\theta\\)</span> revient à minimiser la somme des\ncarrés <span class=\"math inline\">\\(\\sum_{i=1}^n (X_i -\n\\theta)^2\\)</span>. La dérivée par rapport à <span\nclass=\"math inline\">\\(\\theta\\)</span> est <span\nclass=\"math inline\">\\(\\frac{d}{d\\theta} \\sum_{i=1}^n (X_i - \\theta)^2 =\n\\sum_{i=1}^n -2(X_i - \\theta)\\)</span>. En annulant la dérivée, on\nobtient <span class=\"math inline\">\\(\\sum_{i=1}^n (X_i - \\theta) =\n0\\)</span>, soit <span class=\"math inline\">\\(\\sum X_i - n\\theta =\n0\\)</span>, d’où <span class=\"math inline\">\\(\\theta = \\frac{1}{n} \\sum\nX_i = \\bar{X}_n\\)</span>. <span class=\"math inline\">\\(\\hat{\\theta}_{MV}\n= \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\)</span></p>\n"
      }
    },
    {
      "id": "9ebbadb8-c3e8-4493-a29d-07bf6c564768",
      "type": "question",
      "value": {
        "latex": "\\( f(x) = \\frac{1}{2} e^{-|x|} \\) (loi de Laplace) ;",
        "html": "<p><span class=\"math inline\">\\(f(x) = \\frac{1}{2} e^{-|x|}\\)</span> (loi\nde Laplace) ;</p>\n"
      }
    },
    {
      "id": "4035d5e2-83c0-42f5-8dc9-fbab59bd2a9b",
      "type": "reponse",
      "value": {
        "latex": "La vraisemblance est \\( L(\\theta) = \\prod_{i=1}^n \\frac{1}{2} e^{-|X_i - \\theta|} = (\\frac{1}{2})^n e^{-\\sum_{i=1}^n |X_i - \\theta|} \\).\n\t\n\tLa log-vraisemblance est \\( \\ell(\\theta) = -n \\ln(2) - \\sum_{i=1}^n |X_i - \\theta| \\).\n\tMaximiser \\( \\ell(\\theta) \\) revient à minimiser la somme des valeurs absolues des écarts \\( \\sum_{i=1}^n |X_i - \\theta| \\). \n\t\n\tCette somme est minimisée lorsque \\( \\theta \\) est la médiane empirique de l'échantillon \\( (X_1, \\dots, X_n) \\).\n\t\\( \\hat{\\theta}_{MV} = \\text{med}(X_1, \\dots, X_n) \\), la médiane empirique de l'échantillon.",
        "html": "<p>La vraisemblance est <span class=\"math inline\">\\(L(\\theta) =\n\\prod_{i=1}^n \\frac{1}{2} e^{-|X_i - \\theta|} = (\\frac{1}{2})^n\ne^{-\\sum_{i=1}^n |X_i - \\theta|}\\)</span>.</p>\n<p>La log-vraisemblance est <span class=\"math inline\">\\(\\ell(\\theta) =\n-n \\ln(2) - \\sum_{i=1}^n |X_i - \\theta|\\)</span>. Maximiser <span\nclass=\"math inline\">\\(\\ell(\\theta)\\)</span> revient à minimiser la somme\ndes valeurs absolues des écarts <span class=\"math inline\">\\(\\sum_{i=1}^n\n|X_i - \\theta|\\)</span>.</p>\n<p>Cette somme est minimisée lorsque <span\nclass=\"math inline\">\\(\\theta\\)</span> est la médiane empirique de\nl’échantillon <span class=\"math inline\">\\((X_1, \\dots, X_n)\\)</span>.\n<span class=\"math inline\">\\(\\hat{\\theta}_{MV} = \\text{med}(X_1, \\dots,\nX_n)\\)</span>, la médiane empirique de l’échantillon.</p>\n"
      }
    },
    {
      "id": "bf07fef9-d5b0-4676-8480-010cc36252e4",
      "type": "question",
      "value": {
        "latex": "\\( f(x) = \\frac{3}{4} (1 - x^2) \\) sur \\( [-1, 1] \\).",
        "html": "<p><span class=\"math inline\">\\(f(x) = \\frac{3}{4} (1 - x^2)\\)</span> sur\n<span class=\"math inline\">\\([-1, 1]\\)</span>.</p>\n"
      }
    },
    {
      "id": "7b02e343-501a-47ee-9a19-147007562abd",
      "type": "reponse",
      "value": {
        "latex": "La densité \\( f_\\theta(x) = f(x - \\theta) = \\frac{3}{4} (1 - (x - \\theta)^2) \\) est définie pour \\( x - \\theta \\in [-1, 1] \\), c'est-à-dire \\( \\theta - 1 \\le x \\le \\theta + 1 \\).\n\t\n\tLa vraisemblance est \\( L(\\theta) = \\prod_{i=1}^n \\frac{3}{4} (1 - (X_i - \\theta)^2) \\) si \\( \\theta - 1 \\le X_i \\le \\theta + 1 \\) pour tout \\( i \\), et \\( L(\\theta) = 0 \\) sinon.\n\tLa condition \\( \\theta - 1 \\le X_i \\le \\theta + 1 \\) pour tout \\( i \\) équivaut à \\( \\max_i X_i - 1 \\le \\theta \\le \\min_i X_i + 1 \\). Soient \\( X_{(1)} = \\min_i X_i \\) et \\( X_{(n)} = \\max_i X_i \\). La vraisemblance est non nulle pour \\( \\theta \\in [X_{(n)} - 1, X_{(1)} + 1] \\).\n\t\n\tSur cet intervalle, la log-vraisemblance est : $$ \\ell(\\theta) = n \\ln(3/4) + \\sum_{i=1}^n \\ln(1 - (X_i - \\theta)^2). $$\n\tL'estimateur du maximum de vraisemblance \\( \\hat{\\theta}_{MV} \\) est la valeur de \\( \\theta \\) qui maximise \\( \\sum_{i=1}^n \\ln(1 - (X_i - \\theta)^2) \\) sur l'intervalle \\( [X_{(n)} - 1, X_{(1)} + 1] \\). Il n'y a pas de solution analytique simple en général. \n\t\n\tL'estimateur est défini par cette optimisation.\n\t\\( \\hat{\\theta}_{MV} = \\mathrm{argmax}_{\\theta \\in [X_{(n)}-1, X_{(1)}+1]} \\sum_{i=1}^n \\ln(1 - (X_i - \\theta)^2) \\), où \\( X_{(1)} = \\min_i X_i \\) et \\( X_{(n)} = \\max_i X_i \\).",
        "html": "<p>La densité <span class=\"math inline\">\\(f_\\theta(x) = f(x - \\theta) =\n\\frac{3}{4} (1 - (x - \\theta)^2)\\)</span> est définie pour <span\nclass=\"math inline\">\\(x - \\theta \\in [-1, 1]\\)</span>, c’est-à-dire\n<span class=\"math inline\">\\(\\theta - 1 \\le x \\le \\theta +\n1\\)</span>.</p>\n<p>La vraisemblance est <span class=\"math inline\">\\(L(\\theta) =\n\\prod_{i=1}^n \\frac{3}{4} (1 - (X_i - \\theta)^2)\\)</span> si <span\nclass=\"math inline\">\\(\\theta - 1 \\le X_i \\le \\theta + 1\\)</span> pour\ntout <span class=\"math inline\">\\(i\\)</span>, et <span\nclass=\"math inline\">\\(L(\\theta) = 0\\)</span> sinon. La condition <span\nclass=\"math inline\">\\(\\theta - 1 \\le X_i \\le \\theta + 1\\)</span> pour\ntout <span class=\"math inline\">\\(i\\)</span> équivaut à <span\nclass=\"math inline\">\\(\\max_i X_i - 1 \\le \\theta \\le \\min_i X_i +\n1\\)</span>. Soient <span class=\"math inline\">\\(X_{(1)} = \\min_i\nX_i\\)</span> et <span class=\"math inline\">\\(X_{(n)} = \\max_i\nX_i\\)</span>. La vraisemblance est non nulle pour <span\nclass=\"math inline\">\\(\\theta \\in [X_{(n)} - 1, X_{(1)} +\n1]\\)</span>.</p>\n<p>Sur cet intervalle, la log-vraisemblance est : <span\nclass=\"math display\">\\[\\ell(\\theta) = n \\ln(3/4) + \\sum_{i=1}^n \\ln(1 -\n(X_i - \\theta)^2).\\]</span> L’estimateur du maximum de vraisemblance\n<span class=\"math inline\">\\(\\hat{\\theta}_{MV}\\)</span> est la valeur de\n<span class=\"math inline\">\\(\\theta\\)</span> qui maximise <span\nclass=\"math inline\">\\(\\sum_{i=1}^n \\ln(1 - (X_i - \\theta)^2)\\)</span>\nsur l’intervalle <span class=\"math inline\">\\([X_{(n)} - 1, X_{(1)} +\n1]\\)</span>. Il n’y a pas de solution analytique simple en général.</p>\n<p>L’estimateur est défini par cette optimisation. <span\nclass=\"math inline\">\\(\\hat{\\theta}_{MV} = \\mathrm{argmax}_{\\theta \\in\n[X_{(n)}-1, X_{(1)}+1]} \\sum_{i=1}^n \\ln(1 - (X_i - \\theta)^2)\\)</span>,\noù <span class=\"math inline\">\\(X_{(1)} = \\min_i X_i\\)</span> et <span\nclass=\"math inline\">\\(X_{(n)} = \\max_i X_i\\)</span>.</p>\n"
      }
    }
  ],
  "preview": "<p>Soit <span class=\"math inline\">\\(X_1, \\dots, X_n\\)</span> i.i.d. de\ndensité commune <span class=\"math inline\">\\(f_\\theta(x) = f(x -\n\\theta)\\)</span ..."
}