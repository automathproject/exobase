{
  "uuid": "ycEr",
  "titre": "Regression linéaire",
  "theme": [
    "optimisation"
  ],
  "niveau": "",
  "metadata": {
    "exo7id": "",
    "auteur": "Erwan HILLION",
    "createdAt": "2024-10-01",
    "hasIndication": false,
    "hasCorrection": true,
    "youtube": "",
    "chapitre": "Optimisation",
    "sousChapitre": "Autre",
    "organisation": "AMSCC",
    "updatedAt": "2025-03-24T17:17:24.298Z",
    "resume": "Cet exercice porte sur la régression linéaire simple et l'optimisation du risque quadratique $R(a, b) = \\sum_{i=1}^n (y_i - (ax_i + b))^2$. Il vise à évaluer les compétences suivantes : \n\n1.  Développement et simplification d'expressions mathématiques pour exprimer $R(a,b)$ sous une forme polynomiale.\n2.  Calcul du gradient de $R(a, b)$, soit $\\nabla R(a,b)$.\n3.  Détermination du point critique $(a^*, b^*)$ en résolvant $\\nabla R(a^*, b^*) = 0$, ce qui implique la résolution d'un système d'équations linéaires.\n4.  Calcul de la matrice Hessienne de $R(a, b)$, soit $\\textrm{Hess}_R(a,b)$.\n5.  Démonstration de la convexité de la fonction $R$ en utilisant la Hessienne et l'inégalité de Cauchy-Schwarz.",
    "competences": [
      "calculer un gradient vectoriel",
      "calculer une matrice hessienne",
      "résoudre un système d'équations linéaires",
      "appliquer l'inégalité de Cauchy-Schwarz",
      "développer une expression polynomiale"
    ],
    "niveau_difficulte": "intermédiaire",
    "mots_cles": [
      "regression linéaire",
      "risque quadratique",
      "optimisation",
      "gradient",
      "hessienne",
      "point critique",
      "convexité",
      "Cauchy-Schwarz"
    ],
    "concepts_fondamentaux": [
      "optimisation de fonction à plusieurs variables",
      "calcul différentiel (gradient, hessienne)",
      "convexité",
      "systèmes d'équations linéaires"
    ],
    "prerequis": [
      "calcul différentiel de base (dérivées partielles)",
      "algèbre linéaire (systèmes d'équations, produit scalaire)",
      "notions de base sur l'optimisation"
    ],
    "type_exercice": "Problème à étapes",
    "temps_estime": "60-90 minutes"
  },
  "contenu": [
    {
      "id": "15df66dc-a0b9-4e4e-8282-e22b3ab87ca7",
      "type": "description",
      "value": {
        "latex": "On dispose d'observations $(x_1,y_1),\\ldots,(x_n,y_n)$. On cherche les \"meilleurs\" coefficients $a$ et $b$ tels que pour chaque observation, on ait $y_i \\approx a x_i + b$. Ce probl\\`eme est appel\\'e r\\'egression lin\\'eaire simple.\n\n\\medskip\n\nPour mesurer la qualit\\'e des param\\`etres $(a,b)$, on souhaite que l'\\'ecart entre $y_i$ et $ax_i+b$ soit faible pour chaque observation. Pour quantifier l'erreur, on utilise le risque quadratique :\n\\begin{equation*}\nR(a,b) = \\sum_{i=1}^n (y_i - (a x_i+b) )^2.\n\\end{equation*}\nLe probl\\`eme est de minimiser la fonction $R(a,b)$.",
        "html": "<p>On dispose d’observations <span\nclass=\"math inline\">\\((x_1,y_1),\\ldots,(x_n,y_n)\\)</span>. On cherche\nles \"meilleurs\" coefficients <span class=\"math inline\">\\(a\\)</span> et\n<span class=\"math inline\">\\(b\\)</span> tels que pour chaque observation,\non ait <span class=\"math inline\">\\(y_i \\approx a x_i + b\\)</span>. Ce\nproblème est appelé régression linéaire simple.</p>\n<p>Pour mesurer la qualité des paramètres <span\nclass=\"math inline\">\\((a,b)\\)</span>, on souhaite que l’écart entre\n<span class=\"math inline\">\\(y_i\\)</span> et <span\nclass=\"math inline\">\\(ax_i+b\\)</span> soit faible pour chaque\nobservation. Pour quantifier l’erreur, on utilise le risque quadratique\n: <span class=\"math display\">\\[R(a,b) = \\sum_{i=1}^n (y_i - (a x_i+b)\n)^2.\\]</span> Le problème est de minimiser la fonction <span\nclass=\"math inline\">\\(R(a,b)\\)</span>.</p>\n"
      }
    },
    {
      "id": "ec4cb490-97d0-4b0a-9535-c38703888c00",
      "type": "question",
      "value": {
        "latex": "Montrer que :\n\\begin{equation*}\nR(a,b) = a^2 \\sum_{i=1}^n x_i^2 + 2ab \\sum_{i=1}^n x_i + b^2 n -2 a\\sum_{i=1}^n x_i y_i -2 b \\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2.\n\\end{equation*}",
        "html": "<p>Montrer que : <span class=\"math display\">\\[R(a,b) = a^2 \\sum_{i=1}^n\nx_i^2 + 2ab \\sum_{i=1}^n x_i + b^2 n -2 a\\sum_{i=1}^n x_i y_i -2 b\n\\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2.\\]</span></p>\n"
      }
    },
    {
      "id": "bfb02e64-4057-43e7-ab68-f614e9ce04ab",
      "type": "reponse",
      "value": {
        "latex": "La formule s'obtient simplement en développant chaque expression de la somme.",
        "html": "<p>La formule s’obtient simplement en développant chaque expression de\nla somme.</p>\n"
      }
    },
    {
      "id": "863bb77f-e057-4300-b15c-7ced97582e37",
      "type": "question",
      "value": {
        "latex": "Montrer que le gradient de $R$ s'écrit : \n\\begin{equation} \\label{eq:nablaR}\n(\\nabla R)(a,b) = \\left( \\begin{array}{c} 2a \\sum_{i=1}^n x_i^2 + 2 b \\sum_{i=1}^n x_i -2 \\sum_{i=1}^n x_i y_i \\\\ 2a \\sum_{i=1}^n x_i +2 b n -2 \\sum_{i=1}^n y_i \\end{array} \\right).\n\\end{equation}",
        "html": "<p>Montrer que le gradient de <span class=\"math inline\">\\(R\\)</span>\ns’écrit : <span class=\"math display\">\\[\\label{eq:nablaR}\n(\\nabla R)(a,b) = \\left( \\begin{array}{c} 2a \\sum_{i=1}^n x_i^2 + 2 b\n\\sum_{i=1}^n x_i -2 \\sum_{i=1}^n x_i y_i \\\\ 2a \\sum_{i=1}^n x_i +2 b n\n-2 \\sum_{i=1}^n y_i \\end{array} \\right).\\]</span></p>\n"
      }
    },
    {
      "id": "bdd53892-315f-46cd-90c5-a5ee4ead5f61",
      "type": "reponse",
      "value": {
        "latex": "Cela découle d'un calcul direct.",
        "html": "<p>Cela découle d’un calcul direct.</p>\n"
      }
    },
    {
      "id": "181b00c3-0392-4292-a12f-6be9d32c987c",
      "type": "question",
      "value": {
        "latex": "Montrer que $R$ possède un unique point critique $(a^*,b^*)$ que l'on exprimera à l'aide des $x_i$ et des $y_i$.",
        "html": "<p>Montrer que <span class=\"math inline\">\\(R\\)</span> possède un unique\npoint critique <span class=\"math inline\">\\((a^*,b^*)\\)</span> que l’on\nexprimera à l’aide des <span class=\"math inline\">\\(x_i\\)</span> et des\n<span class=\"math inline\">\\(y_i\\)</span>.</p>\n"
      }
    },
    {
      "id": "2ddb8c1c-4186-45bb-9553-54680e3e5a60",
      "type": "reponse",
      "value": {
        "latex": "On cherche $(a^*,b^*)$ tel que $\\nabla R(a^*,b^*)=0$. Au vu de l'expression explicite du gradient, on s'aperçoit qu'il faut résoudre un système linéaire à deux inconnues. La résolution de ce système (par exemple avec le pivot de Gauss) donne l'unique solution : $$a^* = \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i - n \\sum_{i=1}^n x_iy_i}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n x_i^2} \\ , \\ b^*=\\frac{\\sum_{i=1}^n x_iy_i \\sum_{i=1}^n x_i-\\sum_{i=1}^n y_i \\sum_{i=1}^n x_i^2}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n x_i^2}.$$",
        "html": "<p>On cherche <span class=\"math inline\">\\((a^*,b^*)\\)</span> tel que\n<span class=\"math inline\">\\(\\nabla R(a^*,b^*)=0\\)</span>. Au vu de\nl’expression explicite du gradient, on s’aperçoit qu’il faut résoudre un\nsystème linéaire à deux inconnues. La résolution de ce système (par\nexemple avec le pivot de Gauss) donne l’unique solution : <span\nclass=\"math display\">\\[a^* = \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i - n\n\\sum_{i=1}^n x_iy_i}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n\nx_i^2} \\ , \\ b^*=\\frac{\\sum_{i=1}^n x_iy_i \\sum_{i=1}^n x_i-\\sum_{i=1}^n\ny_i \\sum_{i=1}^n x_i^2}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n\nx_i^2}.\\]</span></p>\n"
      }
    },
    {
      "id": "a59e83ed-575a-4220-b175-b9e1193648b5",
      "type": "question",
      "value": {
        "latex": "Montrer que la hessienne de $R$ s'écrit :\n\\begin{equation*}\n\\textrm{Hess}_R(a,b)  = \\left( \\begin{array}{cc} 2 \\sum_{i=1}^n x_i^2 & 2 \\sum_{i=1}^n x_i \\\\ 2 \\sum_{i=1}^n x_i & 2 n \\end{array}\\right).\n\\end{equation*}",
        "html": "<p>Montrer que la hessienne de <span class=\"math inline\">\\(R\\)</span>\ns’écrit : <span class=\"math display\">\\[\\textrm{Hess}_R(a,b)  = \\left(\n\\begin{array}{cc} 2 \\sum_{i=1}^n x_i^2 &amp; 2 \\sum_{i=1}^n x_i \\\\ 2\n\\sum_{i=1}^n x_i &amp; 2 n \\end{array}\\right).\\]</span></p>\n"
      }
    },
    {
      "id": "46c6968c-40fe-485c-a7f7-6e901b7fd80e",
      "type": "reponse",
      "value": {
        "latex": "La hessienne s'obtient par calcul direct, en dérivant les dérivées partielles obtenues dans le calcul du gradient.",
        "html": "<p>La hessienne s’obtient par calcul direct, en dérivant les dérivées\npartielles obtenues dans le calcul du gradient.</p>\n"
      }
    },
    {
      "id": "5952ca3f-914f-4719-b9e4-339982620ce5",
      "type": "reponse",
      "value": {
        "latex": "On peut remarquer que la hessienne est constante en $a$ et $b$ (car la fonciton $R$ est polynomiale de degré $2$). Il suffit donc de montrer que $\\langle X, H X \\rangle \\ge 0$ pour tout vecteur $X = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\in \\R^2$, où $H$ est la hessienne obtenue précédemment. Ce produit scalaire s'écrit, après développement : $$ \\frac{1}{4} \\langle X, H X \\rangle = a^2 \\sum_{i=1}^n x_i^2 + 2 a b \\sum_{i=1}^n x_i  + b^2 n.$$ Par Cauchy-Schwarz, on a $\\sum_{i=1}^n x_i^2 \\ge \\frac{1}{n} \\left(\\sum_{i=1}^n x_i \\right)^2$, d'où il vient : $$ \\frac{1}{4} \\langle X, H X \\rangle  \\ge \\frac{1}{n}\\left(a \\sum_{i=1}^n x_i + b n \\right)^2 \\ge 0,$$ comme voulu.",
        "html": "<p>On peut remarquer que la hessienne est constante en <span\nclass=\"math inline\">\\(a\\)</span> et <span\nclass=\"math inline\">\\(b\\)</span> (car la fonciton <span\nclass=\"math inline\">\\(R\\)</span> est polynomiale de degré <span\nclass=\"math inline\">\\(2\\)</span>). Il suffit donc de montrer que <span\nclass=\"math inline\">\\(\\langle X, H X \\rangle \\ge 0\\)</span> pour tout\nvecteur <span class=\"math inline\">\\(X = \\begin{pmatrix} a \\\\ b\n\\end{pmatrix} \\in \\R^2\\)</span>, où <span\nclass=\"math inline\">\\(H\\)</span> est la hessienne obtenue précédemment.\nCe produit scalaire s’écrit, après développement : <span\nclass=\"math display\">\\[\\frac{1}{4} \\langle X, H X \\rangle = a^2\n\\sum_{i=1}^n x_i^2 + 2 a b \\sum_{i=1}^n x_i  + b^2 n.\\]</span> Par\nCauchy-Schwarz, on a <span class=\"math inline\">\\(\\sum_{i=1}^n x_i^2 \\ge\n\\frac{1}{n} \\left(\\sum_{i=1}^n x_i \\right)^2\\)</span>, d’où il vient :\n<span class=\"math display\">\\[\\frac{1}{4} \\langle X, H X \\rangle  \\ge\n\\frac{1}{n}\\left(a \\sum_{i=1}^n x_i + b n \\right)^2 \\ge 0,\\]</span>\ncomme voulu.</p>\n"
      }
    }
  ],
  "preview": "<p>On dispose d’observations <span\nclass=\"math inline\">\\((x_1,y_1),\\ldots,(x_n,y_n)\\)</span>. On cherche\nles \"meilleurs\" coefficients <span class=\"mat ..."
}