{
  "uuid": "ycEr",
  "titre": "Regression linéaire",
  "theme": [
    "optimisation"
  ],
  "niveau": "",
  "metadata": {
    "exo7id": "",
    "auteur": "Erwan HILLION",
    "createdAt": "2024-10-01",
    "hasIndication": false,
    "hasCorrection": true,
    "youtube": "",
    "chapitre": "",
    "sousChapitre": "",
    "organisation": "AMSCC",
    "updatedAt": "2025-03-18T17:56:30.357Z"
  },
  "contenu": [
    {
      "id": "e9a78676-cf9e-4d33-870d-a5ca03fe8cff",
      "type": "description",
      "value": {
        "latex": "On dispose d'observations $(x_1,y_1),\\ldots,(x_n,y_n)$. On cherche les \"meilleurs\" coefficients $a$ et $b$ tels que pour chaque observation, on ait $y_i \\approx a x_i + b$. Ce probl\\`eme est appel\\'e r\\'egression lin\\'eaire simple.\n\n\\medskip\n\nPour mesurer la qualit\\'e des param\\`etres $(a,b)$, on souhaite que l'\\'ecart entre $y_i$ et $ax_i+b$ soit faible pour chaque observation. Pour quantifier l'erreur, on utilise le risque quadratique :\n\\begin{equation*}\nR(a,b) = \\sum_{i=1}^n (y_i - (a x_i+b) )^2.\n\\end{equation*}\nLe probl\\`eme est de minimiser la fonction $R(a,b)$.",
        "html": "<p>On dispose d’observations <span\nclass=\"math inline\">\\((x_1,y_1),\\ldots,(x_n,y_n)\\)</span>. On cherche\nles \"meilleurs\" coefficients <span class=\"math inline\">\\(a\\)</span> et\n<span class=\"math inline\">\\(b\\)</span> tels que pour chaque observation,\non ait <span class=\"math inline\">\\(y_i \\approx a x_i + b\\)</span>. Ce\nproblème est appelé régression linéaire simple.</p>\n<p>Pour mesurer la qualité des paramètres <span\nclass=\"math inline\">\\((a,b)\\)</span>, on souhaite que l’écart entre\n<span class=\"math inline\">\\(y_i\\)</span> et <span\nclass=\"math inline\">\\(ax_i+b\\)</span> soit faible pour chaque\nobservation. Pour quantifier l’erreur, on utilise le risque quadratique\n: <span class=\"math display\">\\[R(a,b) = \\sum_{i=1}^n (y_i - (a x_i+b)\n)^2.\\]</span> Le problème est de minimiser la fonction <span\nclass=\"math inline\">\\(R(a,b)\\)</span>.</p>\n"
      }
    },
    {
      "id": "dd88dda6-d2bc-4ca7-b14e-eb4b8e41e00f",
      "type": "question",
      "value": {
        "latex": "Montrer que :\n\\begin{equation*}\nR(a,b) = a^2 \\sum_{i=1}^n x_i^2 + 2ab \\sum_{i=1}^n x_i + b^2 n -2 a\\sum_{i=1}^n x_i y_i -2 b \\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2.\n\\end{equation*}",
        "html": "<p>Montrer que : <span class=\"math display\">\\[R(a,b) = a^2 \\sum_{i=1}^n\nx_i^2 + 2ab \\sum_{i=1}^n x_i + b^2 n -2 a\\sum_{i=1}^n x_i y_i -2 b\n\\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2.\\]</span></p>\n"
      }
    },
    {
      "id": "6693c23c-49ef-480f-abc1-da9118a3593d",
      "type": "reponse",
      "value": {
        "latex": "La formule s'obtient simplement en développant chaque expression de la somme.",
        "html": "<p>La formule s’obtient simplement en développant chaque expression de\nla somme.</p>\n"
      }
    },
    {
      "id": "82967353-526c-4795-b939-b9bd89dcf970",
      "type": "question",
      "value": {
        "latex": "Montrer que le gradient de $R$ s'écrit : \n\\begin{equation} \\label{eq:nablaR}\n(\\nabla R)(a,b) = \\left( \\begin{array}{c} 2a \\sum_{i=1}^n x_i^2 + 2 b \\sum_{i=1}^n x_i -2 \\sum_{i=1}^n x_i y_i \\\\ 2a \\sum_{i=1}^n x_i +2 b n -2 \\sum_{i=1}^n y_i \\end{array} \\right).\n\\end{equation}",
        "html": "<p>Montrer que le gradient de <span class=\"math inline\">\\(R\\)</span>\ns’écrit : <span class=\"math display\">\\[\\label{eq:nablaR}\n(\\nabla R)(a,b) = \\left( \\begin{array}{c} 2a \\sum_{i=1}^n x_i^2 + 2 b\n\\sum_{i=1}^n x_i -2 \\sum_{i=1}^n x_i y_i \\\\ 2a \\sum_{i=1}^n x_i +2 b n\n-2 \\sum_{i=1}^n y_i \\end{array} \\right).\\]</span></p>\n"
      }
    },
    {
      "id": "c1da1918-afc4-45af-a77b-149a0b6cea15",
      "type": "reponse",
      "value": {
        "latex": "Cela découle d'un calcul direct.",
        "html": "<p>Cela découle d’un calcul direct.</p>\n"
      }
    },
    {
      "id": "5dd35dc3-aaec-4c51-b62c-18ff1d1a9c31",
      "type": "question",
      "value": {
        "latex": "Montrer que $R$ possède un unique point critique $(a^*,b^*)$ que l'on exprimera à l'aide des $x_i$ et des $y_i$.",
        "html": "<p>Montrer que <span class=\"math inline\">\\(R\\)</span> possède un unique\npoint critique <span class=\"math inline\">\\((a^*,b^*)\\)</span> que l’on\nexprimera à l’aide des <span class=\"math inline\">\\(x_i\\)</span> et des\n<span class=\"math inline\">\\(y_i\\)</span>.</p>\n"
      }
    },
    {
      "id": "e35a7c20-9c0c-4de2-8326-dcc5598d7d48",
      "type": "reponse",
      "value": {
        "latex": "On cherche $(a^*,b^*)$ tel que $\\nabla R(a^*,b^*)=0$. Au vu de l'expression explicite du gradient, on s'aperçoit qu'il faut résoudre un système linéaire à deux inconnues. La résolution de ce système (par exemple avec le pivot de Gauss) donne l'unique solution : $$a^* = \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i - n \\sum_{i=1}^n x_iy_i}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n x_i^2} \\ , \\ b^*=\\frac{\\sum_{i=1}^n x_iy_i \\sum_{i=1}^n x_i-\\sum_{i=1}^n y_i \\sum_{i=1}^n x_i^2}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n x_i^2}.$$",
        "html": "<p>On cherche <span class=\"math inline\">\\((a^*,b^*)\\)</span> tel que\n<span class=\"math inline\">\\(\\nabla R(a^*,b^*)=0\\)</span>. Au vu de\nl’expression explicite du gradient, on s’aperçoit qu’il faut résoudre un\nsystème linéaire à deux inconnues. La résolution de ce système (par\nexemple avec le pivot de Gauss) donne l’unique solution : <span\nclass=\"math display\">\\[a^* = \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i - n\n\\sum_{i=1}^n x_iy_i}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n\nx_i^2} \\ , \\ b^*=\\frac{\\sum_{i=1}^n x_iy_i \\sum_{i=1}^n x_i-\\sum_{i=1}^n\ny_i \\sum_{i=1}^n x_i^2}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n\nx_i^2}.\\]</span></p>\n"
      }
    },
    {
      "id": "1bf7fcce-54b6-4ac1-9c3b-3a6a8dfbd892",
      "type": "question",
      "value": {
        "latex": "Montrer que la hessienne de $R$ s'écrit :\n\\begin{equation*}\n\\textrm{Hess}_R(a,b)  = \\left( \\begin{array}{cc} 2 \\sum_{i=1}^n x_i^2 & 2 \\sum_{i=1}^n x_i \\\\ 2 \\sum_{i=1}^n x_i & 2 n \\end{array}\\right).\n\\end{equation*}",
        "html": "<p>Montrer que la hessienne de <span class=\"math inline\">\\(R\\)</span>\ns’écrit : <span class=\"math display\">\\[\\textrm{Hess}_R(a,b)  = \\left(\n\\begin{array}{cc} 2 \\sum_{i=1}^n x_i^2 &amp; 2 \\sum_{i=1}^n x_i \\\\ 2\n\\sum_{i=1}^n x_i &amp; 2 n \\end{array}\\right).\\]</span></p>\n"
      }
    },
    {
      "id": "d33f6567-9606-4adb-b65c-141c7dd88e5b",
      "type": "reponse",
      "value": {
        "latex": "La hessienne s'obtient par calcul direct, en dérivant les dérivées partielles obtenues dans le calcul du gradient.",
        "html": "<p>La hessienne s’obtient par calcul direct, en dérivant les dérivées\npartielles obtenues dans le calcul du gradient.</p>\n"
      }
    },
    {
      "id": "4db981a6-102c-4493-905b-c5e65fdf4eb0",
      "type": "reponse",
      "value": {
        "latex": "On peut remarquer que la hessienne est constante en $a$ et $b$ (car la fonciton $R$ est polynomiale de degré $2$). Il suffit donc de montrer que $\\langle X, H X \\rangle \\ge 0$ pour tout vecteur $X = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\in \\R^2$, où $H$ est la hessienne obtenue précédemment. Ce produit scalaire s'écrit, après développement : $$ \\frac{1}{4} \\langle X, H X \\rangle = a^2 \\sum_{i=1}^n x_i^2 + 2 a b \\sum_{i=1}^n x_i  + b^2 n.$$ Par Cauchy-Schwarz, on a $\\sum_{i=1}^n x_i^2 \\ge \\frac{1}{n} \\left(\\sum_{i=1}^n x_i \\right)^2$, d'où il vient : $$ \\frac{1}{4} \\langle X, H X \\rangle  \\ge \\frac{1}{n}\\left(a \\sum_{i=1}^n x_i + b n \\right)^2 \\ge 0,$$ comme voulu.",
        "html": "<p>On peut remarquer que la hessienne est constante en <span\nclass=\"math inline\">\\(a\\)</span> et <span\nclass=\"math inline\">\\(b\\)</span> (car la fonciton <span\nclass=\"math inline\">\\(R\\)</span> est polynomiale de degré <span\nclass=\"math inline\">\\(2\\)</span>). Il suffit donc de montrer que <span\nclass=\"math inline\">\\(\\langle X, H X \\rangle \\ge 0\\)</span> pour tout\nvecteur <span class=\"math inline\">\\(X = \\begin{pmatrix} a \\\\ b\n\\end{pmatrix} \\in \\R^2\\)</span>, où <span\nclass=\"math inline\">\\(H\\)</span> est la hessienne obtenue précédemment.\nCe produit scalaire s’écrit, après développement : <span\nclass=\"math display\">\\[\\frac{1}{4} \\langle X, H X \\rangle = a^2\n\\sum_{i=1}^n x_i^2 + 2 a b \\sum_{i=1}^n x_i  + b^2 n.\\]</span> Par\nCauchy-Schwarz, on a <span class=\"math inline\">\\(\\sum_{i=1}^n x_i^2 \\ge\n\\frac{1}{n} \\left(\\sum_{i=1}^n x_i \\right)^2\\)</span>, d’où il vient :\n<span class=\"math display\">\\[\\frac{1}{4} \\langle X, H X \\rangle  \\ge\n\\frac{1}{n}\\left(a \\sum_{i=1}^n x_i + b n \\right)^2 \\ge 0,\\]</span>\ncomme voulu.</p>\n"
      }
    }
  ],
  "preview": "<p>On dispose d’observations <span\nclass=\"math inline\">\\((x_1,y_1),\\ldots,(x_n,y_n)\\)</span>. On cherche\nles \"meilleurs\" coefficients <span class=\"mat ..."
}