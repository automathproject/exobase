{
  "uuid": "ReNe",
  "titre": "OPTRN",
  "theme": [
    "réseaux de neurones"
  ],
  "niveau": "",
  "metadata": {
    "exo7id": "",
    "auteur": "",
    "createdAt": "2024-10-11",
    "hasIndication": false,
    "hasCorrection": true,
    "youtube": "",
    "chapitre": "Autre",
    "sousChapitre": "Autre",
    "organisation": "AMSCC",
    "updatedAt": "2025-03-24T17:14:29.319Z",
    "resume": "L'exercice porte sur un réseau de neurones multicouches. Il évalue la capacité à: \n\n1. Déterminer les équations de propagation avant dans un réseau de neurones donné, en exprimant les sorties intermédiaires ($f_{11}, f_{12}, h_{11}, h_{12}, f_{21}$) et la sortie finale ($\\hat{y}$) en fonction des poids ($w_i$) et de l'entrée $x$. La fonction d'activation sigmoïde est utilisée, définie par $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n\n2. Appliquer l'algorithme de rétropropagation du gradient pour calculer les dérivées partielles de la fonction d'erreur $E(w) = (y - \\hat{y})^2$ par rapport aux poids $w_j$, c'est-à-dire $\\frac{\\partial E(w)}{\\partial w_j}$ et  $\\frac{\\partial \\hat{y}}{\\partial w_j}$. Il faut ensuite déduire les mises à jour des poids $\\Delta w_j$ en utilisant la formule $\\Delta w_j = \\alpha (y - \\hat{y}) \\frac{\\partial \\hat{y}}{\\partial w_j}$, où $\\alpha$ est le taux d'apprentissage.",
    "competences": [
      "calculer des dérivées partielles",
      "appliquer l'algorithme de rétropropagation du gradient",
      "déterminer les équations d'un réseau de neurones",
      "interpréter un schéma de réseau de neurones",
      "calculer la fonction sigmoïde"
    ],
    "niveau_difficulte": "intermédiaire",
    "mots_cles": [
      "réseau de neurones",
      "rétropropagation",
      "backpropagation",
      "gradient",
      "dérivée partielle",
      "fonction d'erreur",
      "machine learning",
      "apprentissage automatique"
    ],
    "concepts_fondamentaux": [
      "rétropropagation du gradient",
      "descente de gradient",
      "fonction d'activation (sigmoïde)",
      "réseau de neurones multicouches",
      "fonction d'erreur"
    ],
    "prerequis": [
      "calcul différentiel",
      "dérivation de fonctions composées",
      "algèbre linéaire de base",
      "connaissance de base des réseaux de neurones"
    ],
    "type_exercice": "Problème à étapes",
    "temps_estime": "60-90 minutes"
  },
  "contenu": [
    {
      "id": "30f3ec0e-d431-415d-b9a6-fbfe16c62524",
      "type": "description",
      "value": {
        "latex": "Soit le réseau de neurones multicouches décrit pas le graphe suivant:\n\n\\begin{center}\n\t__TIKZ_e56981b8-6601-4629-a96f-6e6e1770f37d__  \n\\end{center}",
        "html": "<p>Soit le réseau de neurones multicouches décrit pas le graphe\nsuivant:</p>\n<div class=\"center\">\n<p><!-- Conversion error: LaTeX Error:\nUndefined control sequence.\nUndefined control sequence.\nUndefined control sequence. -->\n<!-- Problematic TikZ content:\n[scale=1]\n\\def\\layersep{2cm}\n\\tikzstyle{every pin edge}=[thick]\n\\tikzstyle{neuron}=[circle,fill=black!25,minimum size=12pt,inner sep=0pt]\n\\tikzstyle{entree}=[];\n\\tikzstyle{input neuron}=[neuron, fill=green!50];\n\\tikzstyle{output neuron}=[neuron, fill=red!50];\n\\tikzstyle{hidden neuron}=[neuron, fill=blue!50];\n\\tikzstyle{annot} = [text width=4em, text centered]\n\n% Entree\n\\node[entree,blue] (E-1) at (-\\layersep,-0.5) {$1$};\n\\node[entree,blue] (E-2) at (-\\layersep,-2.5) {$x$};\n\\node[entree,blue] (E-3) at (-\\layersep,-5) {$1$};\n\n% Premiere couche\n%\\node[input neuron] (I-1) at (0,-1.25) {};\n\\node[input neuron] (I-1) at (0,-1.5) {};\n\\node[input neuron] (I-3) at (0,-3.75) {};\n\n\\node[below right=0.8ex,scale=0.7] at (I-1) {$\\Sigma$};\n%\\node[above right=0.8ex,scale=0.7] at (I-2) {$H$};\n\\node[below right=0.8ex,scale=0.7] at (I-3) {$\\Sigma$};\n\n\\node[below right=0.8ex,scale=0.7] at (I-1) {};\n\\node[below right=0.8ex,scale=0.7] at (I-3) {};\n%\\node[below right=0.8ex,scale=0.7] at (I-2) {};\n\n% \\node[above right=0.8ex,blue] at (I-1) {$s_1$};\n% \\node[above right=0.8ex,blue] at (I-2) {$s_2$};\n% \\node[above right=0.8ex,blue] at (I-3) {$s_3$};\n\n%Seconde couche et sortie\n\\node[hidden neuron] (O) at (\\layersep,-3.75 cm) {};\n\\node[hidden neuron] (P) at (\\layersep,-1.5 cm) {};\n\\node[above right=0.8ex,scale=0.7] at (O) {$h_{12}$};\n\\node[right=0.5ex,scale=0.7] at (P) {$h_{11}$};\n\n% Arrete et poids\n\\path[thick] (E-1) edge node[pos=0.5,above,scale=0.7]{$w_{1}$} (I-1) ;\n\\path[thick] (E-2) edge node[pos=0.5,above left,scale=0.7]{$w_{2}$} (I-1);\n\\path[thick] (E-2) edge node[pos=0.5,above,scale=0.7]{$w_4$} (I-3);\n\\path[thick] (E-3) edge node[pos=0.5,above,scale=0.7]{$w_3$} (I-3);\n\\path[thick] (I-1) edge node[pos=0.5,above,scale=0.7]{$f_{11}$} (P);\n\\path[thick] (I-3) edge node[pos=0.5,below,scale=0.7]{$f_{12}$}(O);\n\\node[below right=0.8ex,scale=0.5] at (O) {$\\text{Sigmoide}$};\n\\node[below =0.8ex,scale=0.5] at (P) {$\\text{Sigmoide}$};\n%3eme couche et sortie\n\\node[hidden neuron] (Q) at (2*\\layersep,-2.5 cm) {};\n\\node[output neuron] (R) at (3*\\layersep,-2.5 cm) {};\n\\node[below right=0.8ex,scale=0.7] at (R) {$\\text{Id}$};\n\\node[input neuron] (I-4) at (2.5,-0.5) {$1$};\n\\path[thick] (O) edge node[pos=0.6,above,scale=0.7]{$w_{7}$} (Q) ;\n\\node[below right=0.8ex,scale=0.7] at (Q) {$\\Sigma$};\n\\path[thick] (I-4) edge node[pos=0.5,above,scale=0.7]{$w_{5}$} (Q) ;\n\\path[thick] (Q) edge node[pos=0.5,above,scale=0.7]{$f_{21}$} (R);\n\\path[thick] (P) edge node[pos=0.5,above,scale=0.7]{$w_{6}$} (Q) ;\n%\\path[thick] (E-1) edge node[pos=0.8,above,scale=0.7]{$w_{1}$} (I-1) ;\n\\draw[->,thick] (R)-- ++(2,0) node[right,blue]{$\\hat{y}$};\n %  \\node[hidden neuron] (N) at (2*\\layersep,-2 cm) {}; % Nouvel neurone de la nouvelle couche\n     %           \\node[below right=0.8ex,scale=0.7] at (N) {$h_{21}$}; % Nouveau neurone\n\n% Sortie\n%\\draw[->,thick] (O)-- ++(1,0) node[right,blue]{$F(x,y)$};\n--></p>\n</div>\n"
      }
    },
    {
      "id": "59b40e32-ade4-437d-b36b-442583e9a3bd",
      "type": "question",
      "value": {
        "latex": "Donner les formules qui déterminent les sorties intermédiaires $f_{11},\\,f_{12},\\,h_{11},\\,h_{12}$ et $f_{21}$ ainsi que la sortie finale $\\hat{y}$.",
        "html": "<p>Donner les formules qui déterminent les sorties intermédiaires <span\nclass=\"math inline\">\\(f_{11},\\,f_{12},\\,h_{11},\\,h_{12}\\)</span> et\n<span class=\"math inline\">\\(f_{21}\\)</span> ainsi que la sortie finale\n<span class=\"math inline\">\\(\\hat{y}\\)</span>.</p>\n"
      }
    },
    {
      "id": "b4b1f0b6-0638-4231-b7b4-72fd42aca268",
      "type": "reponse",
      "value": {
        "latex": "\\[\nf_{11} = w_2 x + w_1\n\\]\n\\[\nf_{12} = w_4 x + w_3\n\\]\n\\[\nh_{11} = \\sigma(f_{11}) = \\frac{1}{1 + e^{-f_{11}}}\n\\]\n\\[\nh_{12} = \\sigma(f_{12}) = \\frac{1}{1 + e^{-f_{12}}}\n\\]\n\\[\n\\hat{y} = f_{21} = w_6 h_{11} + w_7 h_{12} + w_5\n\\]",
        "html": "<p><span class=\"math display\">\\[f_{11} = w_2 x + w_1\\]</span> <span\nclass=\"math display\">\\[f_{12} = w_4 x + w_3\\]</span> <span\nclass=\"math display\">\\[h_{11} = \\sigma(f_{11}) = \\frac{1}{1 +\ne^{-f_{11}}}\\]</span> <span class=\"math display\">\\[h_{12} =\n\\sigma(f_{12}) = \\frac{1}{1 + e^{-f_{12}}}\\]</span> <span\nclass=\"math display\">\\[\\hat{y} = f_{21} = w_6 h_{11} + w_7 h_{12} +\nw_5\\]</span></p>\n"
      }
    },
    {
      "id": "3492e201-af9a-4771-b7bf-2de29c0983a4",
      "type": "question",
      "value": {
        "latex": "On pose la fonction d'erreur $E(w)=(y-\\hat{y})^2.$ En appliquant l'algorithme de backpropagation (méthode du Gradient appliqué au réseau de neurones), déterminer les dérivées partielles $\\frac{\\partial E(w)}{\\partial w_j}$ puis $\\frac{\\partial \\hat{y}}{\\partial w_j}$. En déduire les expressions des mises à jour des paramètres $\\Delta w_j$ (variation des poids) pour $j=1,\\cdots{},7$.",
        "html": "<p>On pose la fonction d’erreur <span\nclass=\"math inline\">\\(E(w)=(y-\\hat{y})^2.\\)</span> En appliquant\nl’algorithme de backpropagation (méthode du Gradient appliqué au réseau\nde neurones), déterminer les dérivées partielles <span\nclass=\"math inline\">\\(\\frac{\\partial E(w)}{\\partial w_j}\\)</span> puis\n<span class=\"math inline\">\\(\\frac{\\partial \\hat{y}}{\\partial\nw_j}\\)</span>. En déduire les expressions des mises à jour des\nparamètres <span class=\"math inline\">\\(\\Delta w_j\\)</span> (variation\ndes poids) pour <span class=\"math inline\">\\(j=1,\\cdots{},7\\)</span>.</p>\n"
      }
    },
    {
      "id": "d2eba2f2-157c-459f-8168-f44ec3e32a4d",
      "type": "reponse",
      "value": {
        "latex": "La fonction d’erreur est donnée par :\n\\[\nE(\\mathbf{w}) = (y - \\hat{y})^2\n\\]\n\nDonc, on aura :\n\\[\n\\frac{\\partial E(\\mathbf{w})}{\\partial w_j} = -2(y - \\hat{y}) \\frac{\\partial \\hat{y}}{\\partial w_j}\n\\]\n\nD’après la propagation en avant, on a : $\\hat{y} = f_{21} = w_6 h_{11} + w_7 h_{12} + w_5$.  \nLes dérivées partielles $\\frac{\\partial \\hat{y}}{\\partial w_j}$ peuvent être calculées comme suit :\n\\[\n\\frac{\\partial \\hat{y}}{\\partial w_5} = 1, \\quad\n\\frac{\\partial \\hat{y}}{\\partial w_6} = h_{11}, \\quad\n\\frac{\\partial \\hat{y}}{\\partial w_7} = h_{12}\n\\]\n\nPour les autres paramètres :\n\\[\n\\frac{\\partial \\hat{y}}{\\partial w_1} = \\frac{\\partial \\hat{y}}{\\partial h_{11}} \\times \\frac{\\partial h_{11}}{\\partial f_{11}} \\times \\frac{\\partial f_{11}}{\\partial w_1} = w_6 h_{11}(1 - h_{11})\n\\]\n\\[\n\\frac{\\partial \\hat{y}}{\\partial w_2} = w_6 h_{11}(1 - h_{11}) x\n\\]\n\\[\n\\frac{\\partial \\hat{y}}{\\partial w_3} = w_7 h_{12}(1 - h_{12})\n\\]\n\\[\n\\frac{\\partial \\hat{y}}{\\partial w_4} = w_7 h_{12}(1 - h_{12}) x\n\\]\n\nEn fin, la mise à jour de chaque paramètre $\\Delta w_j$ est donnée par :\n\\[\n\\Delta w_j = \\alpha (y - \\hat{y}) \\frac{\\partial \\hat{y}}{\\partial w_j}\n\\]",
        "html": "<p>La fonction d’erreur est donnée par : <span\nclass=\"math display\">\\[E(\\mathbf{w}) = (y - \\hat{y})^2\\]</span></p>\n<p>Donc, on aura : <span class=\"math display\">\\[\\frac{\\partial\nE(\\mathbf{w})}{\\partial w_j} = -2(y - \\hat{y}) \\frac{\\partial\n\\hat{y}}{\\partial w_j}\\]</span></p>\n<p>D’après la propagation en avant, on a : <span\nclass=\"math inline\">\\(\\hat{y} = f_{21} = w_6 h_{11} + w_7 h_{12} +\nw_5\\)</span>. Les dérivées partielles <span\nclass=\"math inline\">\\(\\frac{\\partial \\hat{y}}{\\partial w_j}\\)</span>\npeuvent être calculées comme suit : <span\nclass=\"math display\">\\[\\frac{\\partial \\hat{y}}{\\partial w_5} = 1, \\quad\n\\frac{\\partial \\hat{y}}{\\partial w_6} = h_{11}, \\quad\n\\frac{\\partial \\hat{y}}{\\partial w_7} = h_{12}\\]</span></p>\n<p>Pour les autres paramètres : <span\nclass=\"math display\">\\[\\frac{\\partial \\hat{y}}{\\partial w_1} =\n\\frac{\\partial \\hat{y}}{\\partial h_{11}} \\times \\frac{\\partial\nh_{11}}{\\partial f_{11}} \\times \\frac{\\partial f_{11}}{\\partial w_1} =\nw_6 h_{11}(1 - h_{11})\\]</span> <span\nclass=\"math display\">\\[\\frac{\\partial \\hat{y}}{\\partial w_2} = w_6\nh_{11}(1 - h_{11}) x\\]</span> <span\nclass=\"math display\">\\[\\frac{\\partial \\hat{y}}{\\partial w_3} = w_7\nh_{12}(1 - h_{12})\\]</span> <span class=\"math display\">\\[\\frac{\\partial\n\\hat{y}}{\\partial w_4} = w_7 h_{12}(1 - h_{12}) x\\]</span></p>\n<p>En fin, la mise à jour de chaque paramètre <span\nclass=\"math inline\">\\(\\Delta w_j\\)</span> est donnée par : <span\nclass=\"math display\">\\[\\Delta w_j = \\alpha (y - \\hat{y}) \\frac{\\partial\n\\hat{y}}{\\partial w_j}\\]</span></p>\n"
      }
    }
  ],
  "preview": "<p>Soit le réseau de neurones multicouches décrit pas le graphe\nsuivant:</p>\n<div class=\"center\">\n<p><!-- Conversion error: LaTeX Error:\nUndefined con ..."
}