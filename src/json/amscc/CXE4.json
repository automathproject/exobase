{
  "uuid": "CXE4",
  "titre": "Rétropropagation à une variable",
  "theme": [
    "réseaux de neurones"
  ],
  "niveau": "",
  "metadata": {
    "exo7id": "",
    "auteur": "Maxime NGUYEN",
    "createdAt": "2024-12-02",
    "hasIndication": "",
    "hasCorrection": "",
    "youtube": "",
    "chapitre": "Autre",
    "sousChapitre": "Autre",
    "organisation": "AMSCC",
    "updatedAt": "2025-04-04T19:02:02.834Z",
    "resume": "Cet exercice porte sur la rétropropagation dans un réseau de neurones à une couche cachée. Il évalue les compétences suivantes :\n\n1.  Calcul de l'activation des neurones en utilisant une fonction sigmoïde $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, en connaissant les poids, biais et entrées.\n2.  Calcul de la sortie du réseau, qui est l'activation du neurone de sortie.\n3.  Calcul de l'erreur du réseau en utilisant l'erreur quadratique moyenne $E = \\frac{1}{2}(y - \\hat{y})^2$.\n4.  Calcul des gradients de l'erreur par rapport aux poids des couches de sortie et cachée en utilisant la règle de la chaîne. Ceci implique le calcul des dérivées partielles $\\frac{\\partial E}{\\partial w_i}$ et l'utilisation de l'erreur locale $\\delta$.\n5.  Mise à jour des poids du réseau en utilisant la descente de gradient avec un taux d'apprentissage donné $\\eta$, suivant la formule $w_i^{\\text{nouveau}} = w_i^{\\text{ancien}} - \\eta \\frac{\\partial E}{\\partial w_i}$.",
    "competences": [
      "calculer les gradients par rétropropagation",
      "appliquer la fonction sigmoïde",
      "calculer la sortie d'un neurone",
      "appliquer la formule de l'erreur quadratique moyenne",
      "mettre à jour les poids d'un réseau de neurones"
    ],
    "niveau_difficulte": "intermédiaire",
    "mots_cles": [
      "réseau de neurones",
      "rétropropagation",
      "gradient",
      "apprentissage",
      "sigmoïde",
      "erreur quadratique moyenne",
      "poids",
      "biais"
    ],
    "concepts_fondamentaux": [
      "réseau de neurones multicouche",
      "fonction d'activation",
      "descente de gradient",
      "rétropropagation du gradient"
    ],
    "prerequis": [
      "calcul différentiel (dérivées)",
      "algèbre linéaire (produit scalaire)",
      "compréhension de base des réseaux de neurones"
    ],
    "type_exercice": "Exercice d'application directe",
    "temps_estime": "45 minutes"
  },
  "contenu": [
    {
      "id": "54508b0d-0aa7-4071-b3ff-4a7a556b1f23",
      "type": "description",
      "value": {
        "latex": "Vous allez travailler avec un réseau de neurones à une couche cachée, conçu pour effectuer une tâche de prédiction simple. \n\t\n\t\n\t\n\t\\textbf{Description du réseau}\n\t\n\t\\begin{itemize}\n\t\t\\item \\textbf{Entrée :}\n\t\t\\begin{itemize}\n\t\t\t\\item \\( x \\in \\mathbb{R} \\) (scalaire)\n\t\t\\end{itemize}\n\t\t\n\t\t\\item \\textbf{Couche cachée (2 neurones) :}\n\t\t\\begin{itemize}\n\t\t\t\\item \\textbf{Neurone caché 1 :}\n\t\t\t\\begin{itemize}\n\t\t\t\t\\item Poids : \\( w_1 = 0{,}15 \\)\n\t\t\t\t\\item Biais : \\( b_1 = 0{,}35 \\)\n\t\t\t\\end{itemize}\n\t\t\t\\item \\textbf{Neurone caché 2 :}\n\t\t\t\\begin{itemize}\n\t\t\t\t\\item Poids : \\( w_2 = 0{,}20 \\)\n\t\t\t\t\\item Biais : \\( b_2 = 0{,}35 \\)\n\t\t\t\\end{itemize}\n\t\t\\end{itemize}\n\t\t\n\t\t\\item \\textbf{Couche de sortie (1 neurone) :}\n\t\t\\begin{itemize}\n\t\t\t\\item \\textbf{Neurone de sortie :}\n\t\t\t\\begin{itemize}\n\t\t\t\t\\item Poids : \\( w_3 = 0{,}40 \\) (connecté au neurone caché 1)\n\t\t\t\t\\item Poids : \\( w_4 = 0{,}45 \\) (connecté au neurone caché 2)\n\t\t\t\t\\item Biais : \\( b_3 = 0{,}60 \\)\n\t\t\t\\end{itemize}\n\t\t\\end{itemize}\n\t\t\n\t\t\\item \\textbf{Fonctions d'activation :}\n\t\t\\begin{itemize}\n\t\t\t\\item Fonction sigmoïde : \\( \\sigma(z) = \\dfrac{1}{1 + e^{-z}} \\)\n\t\t\\end{itemize}\n\t\t\n\t\t\\item \\textbf{Fonction de coût :}\n\t\t\\begin{itemize}\n\t\t\t\\item Erreur quadratique moyenne : \\( E = \\dfrac{1}{2}(y - \\hat{y})^2 \\)\n\t\t\\end{itemize}\n\t\t\n\t\t\\item \\textbf{Données :}\n\t\t\\begin{itemize}\n\t\t\t\\item \\textbf{Entrée :} \\( x = 0{,}05 \\)\n\t\t\t\\item \\textbf{Sortie cible :} \\( y = 0{,}01 \\)\n\t\t\\end{itemize}\n\t\\end{itemize}",
        "html": "<p>Vous allez travailler avec un réseau de neurones à une couche cachée,\nconçu pour effectuer une tâche de prédiction simple.</p>\n<p><strong>Description du réseau</strong></p>\n<ul>\n<li><p><strong>Entrée :</strong></p>\n<ul>\n<li><p><span class=\"math inline\">\\(x \\in \\mathbb{R}\\)</span>\n(scalaire)</p></li>\n</ul></li>\n<li><p><strong>Couche cachée (2 neurones) :</strong></p>\n<ul>\n<li><p><strong>Neurone caché 1 :</strong></p>\n<ul>\n<li><p>Poids : <span class=\"math inline\">\\(w_1 =\n0{,}15\\)</span></p></li>\n<li><p>Biais : <span class=\"math inline\">\\(b_1 =\n0{,}35\\)</span></p></li>\n</ul></li>\n<li><p><strong>Neurone caché 2 :</strong></p>\n<ul>\n<li><p>Poids : <span class=\"math inline\">\\(w_2 =\n0{,}20\\)</span></p></li>\n<li><p>Biais : <span class=\"math inline\">\\(b_2 =\n0{,}35\\)</span></p></li>\n</ul></li>\n</ul></li>\n<li><p><strong>Couche de sortie (1 neurone) :</strong></p>\n<ul>\n<li><p><strong>Neurone de sortie :</strong></p>\n<ul>\n<li><p>Poids : <span class=\"math inline\">\\(w_3 = 0{,}40\\)</span>\n(connecté au neurone caché 1)</p></li>\n<li><p>Poids : <span class=\"math inline\">\\(w_4 = 0{,}45\\)</span>\n(connecté au neurone caché 2)</p></li>\n<li><p>Biais : <span class=\"math inline\">\\(b_3 =\n0{,}60\\)</span></p></li>\n</ul></li>\n</ul></li>\n<li><p><strong>Fonctions d’activation :</strong></p>\n<ul>\n<li><p>Fonction sigmoïde : <span class=\"math inline\">\\(\\sigma(z) =\n\\dfrac{1}{1 + e^{-z}}\\)</span></p></li>\n</ul></li>\n<li><p><strong>Fonction de coût :</strong></p>\n<ul>\n<li><p>Erreur quadratique moyenne : <span class=\"math inline\">\\(E =\n\\dfrac{1}{2}(y - \\hat{y})^2\\)</span></p></li>\n</ul></li>\n<li><p><strong>Données :</strong></p>\n<ul>\n<li><p><strong>Entrée :</strong> <span class=\"math inline\">\\(x =\n0{,}05\\)</span></p></li>\n<li><p><strong>Sortie cible :</strong> <span class=\"math inline\">\\(y =\n0{,}01\\)</span></p></li>\n</ul></li>\n</ul>\n"
      }
    },
    {
      "id": "32fc902d-d301-4127-815d-33ac445fb6e5",
      "type": "question",
      "value": {
        "latex": "Calculer les sorties des neurones de la couche cachée (\\( h_1 \\) et \\( h_2 \\)).",
        "html": "<p>Calculer les sorties des neurones de la couche cachée (<span\nclass=\"math inline\">\\(h_1\\)</span> et <span\nclass=\"math inline\">\\(h_2\\)</span>).</p>\n"
      }
    },
    {
      "id": "b23bdd76-a674-441d-a0f6-2972c47ec7cb",
      "type": "indication",
      "value": {
        "latex": "",
        "html": "\n"
      }
    },
    {
      "id": "0b1d8034-390a-4f7a-813c-1838d60feb3e",
      "type": "reponse",
      "value": {
        "latex": "Neurone caché 1 : \n    \t$$\\begin{align*}\n    \t\\text{Entrée nette : } & net_{h1} = w_1 \\cdot x + b_1 \\\\\n    \t& net_{h1} = 0{,}15 \\times 0{,}05 + 0{,}35 = 0{,}3575 \\\\\n    \t\\text{Sortie : } & h_1 = \\sigma(net_{h1}) = \\dfrac{1}{1 + e^{-0{,}3575}} \\approx 0{,}5889\n    \t\\end{align*}$$\n    \t\n    \t\n    \tNeurone caché 2 :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \t\\text{Entrée nette : } & net_{h2} = w_2 \\cdot x + b_2 \\\\\n    \t& net_{h2} = 0{,}20 \\times 0{,}05 + 0{,}35 = 0{,}36 \\\\\n    \t\\text{Sortie : } & h_2 = \\sigma(net_{h2}) = \\dfrac{1}{1 + e^{-0{,}36}} \\approx 0{,}5890\n    \t\\end{align*}$$",
        "html": "<p>Neurone caché 1 : <span class=\"math display\">\\[\\begin{align*}\n        \\text{Entrée nette : } &amp; net_{h1} = w_1 \\cdot x + b_1 \\\\\n        &amp; net_{h1} = 0{,}15 \\times 0{,}05 + 0{,}35 = 0{,}3575 \\\\\n        \\text{Sortie : } &amp; h_1 = \\sigma(net_{h1}) = \\dfrac{1}{1 +\ne^{-0{,}3575}} \\approx 0{,}5889\n        \\end{align*}\\]</span></p>\n<p>Neurone caché 2 :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        \\text{Entrée nette : } &amp; net_{h2} = w_2 \\cdot x + b_2 \\\\\n        &amp; net_{h2} = 0{,}20 \\times 0{,}05 + 0{,}35 = 0{,}36 \\\\\n        \\text{Sortie : } &amp; h_2 = \\sigma(net_{h2}) = \\dfrac{1}{1 +\ne^{-0{,}36}} \\approx 0{,}5890\n        \\end{align*}\\]</span></p>\n"
      }
    },
    {
      "id": "b69a088e-bc67-45e1-8747-838a747bedfd",
      "type": "question",
      "value": {
        "latex": "Calculer la sortie du neurone de sortie (\\( \\hat{y} \\)).",
        "html": "<p>Calculer la sortie du neurone de sortie (<span\nclass=\"math inline\">\\(\\hat{y}\\)</span>).</p>\n"
      }
    },
    {
      "id": "d3bc1eb9-49a3-40c2-815d-7c1fdb3b63c9",
      "type": "indication",
      "value": {
        "latex": "",
        "html": "\n"
      }
    },
    {
      "id": "4c563430-f3c8-43d8-beb5-fc9776e5dec1",
      "type": "reponse",
      "value": {
        "latex": "$$\\begin{align*}\n    \t\\text{Entrée nette : } & net_o = w_3 \\cdot h_1 + w_4 \\cdot h_2 + b_3 \\\\\n    \t& net_o = 0{,}40 \\times 0{,}5889 + 0{,}45 \\times 0{,}5890 + 0{,}60 \\\\\n    \t& net_o = 0{,}2356 + 0{,}2650 + 0{,}60 = 1{,}1006 \\\\\n    \t\\text{Sortie : } & \\hat{y} = \\sigma(net_o) = \\dfrac{1}{1 + e^{-1{,}1006}} \\approx 0{,}7507\n    \t\\end{align*}$$",
        "html": "<p><span class=\"math display\">\\[\\begin{align*}\n        \\text{Entrée nette : } &amp; net_o = w_3 \\cdot h_1 + w_4 \\cdot\nh_2 + b_3 \\\\\n        &amp; net_o = 0{,}40 \\times 0{,}5889 + 0{,}45 \\times 0{,}5890 +\n0{,}60 \\\\\n        &amp; net_o = 0{,}2356 + 0{,}2650 + 0{,}60 = 1{,}1006 \\\\\n        \\text{Sortie : } &amp; \\hat{y} = \\sigma(net_o) = \\dfrac{1}{1 +\ne^{-1{,}1006}} \\approx 0{,}7507\n        \\end{align*}\\]</span></p>\n"
      }
    },
    {
      "id": "d7b1540e-49c1-48f4-ae66-8afb8b13b29e",
      "type": "question",
      "value": {
        "latex": "Calculer l'erreur du réseau en utilisant la fonction de coût.",
        "html": "<p>Calculer l’erreur du réseau en utilisant la fonction de coût.</p>\n"
      }
    },
    {
      "id": "a4320b49-0957-4a94-b021-0b5a6967f9ff",
      "type": "reponse",
      "value": {
        "latex": "$$\\begin{align*}\n    \tE = \\dfrac{1}{2}(y - \\hat{y})^2 = \\dfrac{1}{2}(0{,}01 - 0{,}7507)^2 = \\dfrac{1}{2}(-0{,}7407)^2 \\approx 0{,}2741\n    \t\\end{align*}$$",
        "html": "<p><span class=\"math display\">\\[\\begin{align*}\n        E = \\dfrac{1}{2}(y - \\hat{y})^2 = \\dfrac{1}{2}(0{,}01 -\n0{,}7507)^2 = \\dfrac{1}{2}(-0{,}7407)^2 \\approx 0{,}2741\n        \\end{align*}\\]</span></p>\n"
      }
    },
    {
      "id": "9cbc890a-4603-4280-b8e8-b459de139cdd",
      "type": "question",
      "value": {
        "latex": "Calculer les gradients des poids \\( w_3 \\) et \\( w_4 \\) de la couche de sortie  les gradients des poids \\( w_1 \\) et \\( w_2 \\) de la couche cachée.",
        "html": "<p>Calculer les gradients des poids <span\nclass=\"math inline\">\\(w_3\\)</span> et <span\nclass=\"math inline\">\\(w_4\\)</span> de la couche de sortie les gradients\ndes poids <span class=\"math inline\">\\(w_1\\)</span> et <span\nclass=\"math inline\">\\(w_2\\)</span> de la couche cachée.</p>\n"
      }
    },
    {
      "id": "6dd3ce18-550f-4177-a354-5a227441ccbd",
      "type": "reponse",
      "value": {
        "latex": "Calcul de l'erreur locale au neurone de sortie (\\( \\delta_o \\)) :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \t\\delta_o = \\dfrac{\\partial E}{\\partial net_o} = \\dfrac{\\partial E}{\\partial \\hat{y}} \\cdot \\dfrac{\\partial \\hat{y}}{\\partial net_o} \\\\\n    \t\\dfrac{\\partial E}{\\partial \\hat{y}} = \\hat{y} - y = 0{,}7507 - 0{,}01 = 0{,}7407 \\\\\n    \t\\dfrac{\\partial \\hat{y}}{\\partial net_o} = \\hat{y}(1 - \\hat{y}) = 0{,}7507 \\times 0{,}2493 \\approx 0{,}1875 \\\\\n    \t\\delta_o = 0{,}7407 \\times 0{,}1875 \\approx 0{,}1389\n    \t\\end{align*}$$\n    \t\n    \t\n    \tCalcul des gradients des poids :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \t\\dfrac{\\partial E}{\\partial w_3} = \\delta_o \\cdot h_1 = 0{,}1389 \\times 0{,}5889 \\approx 0{,}0817 \\\\\n    \t\\dfrac{\\partial E}{\\partial w_4} = \\delta_o \\cdot h_2 = 0{,}1389 \\times 0{,}5890 \\approx 0{,}0818\n    \t\\end{align*}$$\n    \t\n    \t\n    \tCalcul des gradients des poids \\( w_1 \\) et \\( w_2 \\)\n    \t\n    \tCalcul des erreurs locales aux neurones cachés (\\( \\delta_{h1} \\) et \\( \\delta_{h2} \\)) :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \t\\delta_{h1} = \\delta_o \\cdot w_3 \\cdot h_1 (1 - h_1) \\\\\n    \t& = 0{,}1389 \\times 0{,}40 \\times 0{,}5889 \\times (1 - 0{,}5889) \\\\\n    \t& \\approx 0{,}1389 \\times 0{,}40 \\times 0{,}5889 \\times 0{,}4111 \\approx 0{,}0134 \\\\\n    \t\\delta_{h2} = \\delta_o \\cdot w_4 \\cdot h_2 (1 - h_2) \\\\\n    \t& = 0{,}1389 \\times 0{,}45 \\times 0{,}5890 \\times (1 - 0{,}5890) \\\\\n    \t& \\approx 0{,}1389 \\times 0{,}45 \\times 0{,}5890 \\times 0{,}4110 \\approx 0{,}0150\n    \t\\end{align*}$$\n    \t\n    \t\n    \tCalcul des gradients des poids :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \t\\dfrac{\\partial E}{\\partial w_1} = \\delta_{h1} \\cdot x = 0{,}0134 \\times 0{,}05 \\approx 0{,}0007 \\\\\n    \t\\dfrac{\\partial E}{\\partial w_2} = \\delta_{h2} \\cdot x = 0{,}0150 \\times 0{,}05 \\approx 0{,}0008\n    \t\\end{align*}$$",
        "html": "<p>Calcul de l’erreur locale au neurone de sortie (<span\nclass=\"math inline\">\\(\\delta_o\\)</span>) :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        \\delta_o = \\dfrac{\\partial E}{\\partial net_o} = \\dfrac{\\partial\nE}{\\partial \\hat{y}} \\cdot \\dfrac{\\partial \\hat{y}}{\\partial net_o} \\\\\n        \\dfrac{\\partial E}{\\partial \\hat{y}} = \\hat{y} - y = 0{,}7507 -\n0{,}01 = 0{,}7407 \\\\\n        \\dfrac{\\partial \\hat{y}}{\\partial net_o} = \\hat{y}(1 - \\hat{y})\n= 0{,}7507 \\times 0{,}2493 \\approx 0{,}1875 \\\\\n        \\delta_o = 0{,}7407 \\times 0{,}1875 \\approx 0{,}1389\n        \\end{align*}\\]</span></p>\n<p>Calcul des gradients des poids :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        \\dfrac{\\partial E}{\\partial w_3} = \\delta_o \\cdot h_1 = 0{,}1389\n\\times 0{,}5889 \\approx 0{,}0817 \\\\\n        \\dfrac{\\partial E}{\\partial w_4} = \\delta_o \\cdot h_2 = 0{,}1389\n\\times 0{,}5890 \\approx 0{,}0818\n        \\end{align*}\\]</span></p>\n<p>Calcul des gradients des poids <span\nclass=\"math inline\">\\(w_1\\)</span> et <span\nclass=\"math inline\">\\(w_2\\)</span></p>\n<p>Calcul des erreurs locales aux neurones cachés (<span\nclass=\"math inline\">\\(\\delta_{h1}\\)</span> et <span\nclass=\"math inline\">\\(\\delta_{h2}\\)</span>) :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        \\delta_{h1} = \\delta_o \\cdot w_3 \\cdot h_1 (1 - h_1) \\\\\n        &amp; = 0{,}1389 \\times 0{,}40 \\times 0{,}5889 \\times (1 -\n0{,}5889) \\\\\n        &amp; \\approx 0{,}1389 \\times 0{,}40 \\times 0{,}5889 \\times\n0{,}4111 \\approx 0{,}0134 \\\\\n        \\delta_{h2} = \\delta_o \\cdot w_4 \\cdot h_2 (1 - h_2) \\\\\n        &amp; = 0{,}1389 \\times 0{,}45 \\times 0{,}5890 \\times (1 -\n0{,}5890) \\\\\n        &amp; \\approx 0{,}1389 \\times 0{,}45 \\times 0{,}5890 \\times\n0{,}4110 \\approx 0{,}0150\n        \\end{align*}\\]</span></p>\n<p>Calcul des gradients des poids :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        \\dfrac{\\partial E}{\\partial w_1} = \\delta_{h1} \\cdot x =\n0{,}0134 \\times 0{,}05 \\approx 0{,}0007 \\\\\n        \\dfrac{\\partial E}{\\partial w_2} = \\delta_{h2} \\cdot x =\n0{,}0150 \\times 0{,}05 \\approx 0{,}0008\n        \\end{align*}\\]</span></p>\n"
      }
    },
    {
      "id": "b8b63eca-200f-4311-a3d4-69ff249c8ffe",
      "type": "question",
      "value": {
        "latex": "Avec un taux d'apprentissage \\( \\eta = 0{,}5 \\), mettre à jour tous les poids du réseau.",
        "html": "<p>Avec un taux d’apprentissage <span class=\"math inline\">\\(\\eta =\n0{,}5\\)</span>, mettre à jour tous les poids du réseau.</p>\n"
      }
    },
    {
      "id": "0c32b685-e18a-422e-ac28-5392f9f7bfe4",
      "type": "reponse",
      "value": {
        "latex": "Avec \\( \\eta = 0{,}5 \\) :\n    \t\n    \t- Mise à jour des poids de la couche de sortie :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \tw_3^{\\text{nouveau}} = w_3^{\\text{ancien}} - \\eta \\dfrac{\\partial E}{\\partial w_3} = 0{,}40 - 0{,}5 \\times 0{,}0817 = 0{,}3592 \\\\\n    \tw_4^{\\text{nouveau}} = w_4^{\\text{ancien}} - \\eta \\dfrac{\\partial E}{\\partial w_4} = 0{,}45 - 0{,}5 \\times 0{,}0818 = 0{,}4091\n    \t\\end{align*}$$\n    \t\n    \t\n    \t- Mise à jour des poids de la couche cachée :\n    \t\n    \t\n    \t$$\\begin{align*}\n    \tw_1^{\\text{nouveau}} = w_1^{\\text{ancien}} - \\eta \\dfrac{\\partial E}{\\partial w_1} = 0{,}15 - 0{,}5 \\times 0{,}0007 = 0{,}1497 \\\\\n    \tw_2^{\\text{nouveau}} = w_2^{\\text{ancien}} - \\eta \\dfrac{\\partial E}{\\partial w_2} = 0{,}20 - 0{,}5 \\times 0{,}0008 = 0{,}1996\n    \t\\end{align*}$$",
        "html": "<p>Avec <span class=\"math inline\">\\(\\eta = 0{,}5\\)</span> :</p>\n<p>- Mise à jour des poids de la couche de sortie :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        w_3^{\\text{nouveau}} = w_3^{\\text{ancien}} - \\eta\n\\dfrac{\\partial E}{\\partial w_3} = 0{,}40 - 0{,}5 \\times 0{,}0817 =\n0{,}3592 \\\\\n        w_4^{\\text{nouveau}} = w_4^{\\text{ancien}} - \\eta\n\\dfrac{\\partial E}{\\partial w_4} = 0{,}45 - 0{,}5 \\times 0{,}0818 =\n0{,}4091\n        \\end{align*}\\]</span></p>\n<p>- Mise à jour des poids de la couche cachée :</p>\n<p><span class=\"math display\">\\[\\begin{align*}\n        w_1^{\\text{nouveau}} = w_1^{\\text{ancien}} - \\eta\n\\dfrac{\\partial E}{\\partial w_1} = 0{,}15 - 0{,}5 \\times 0{,}0007 =\n0{,}1497 \\\\\n        w_2^{\\text{nouveau}} = w_2^{\\text{ancien}} - \\eta\n\\dfrac{\\partial E}{\\partial w_2} = 0{,}20 - 0{,}5 \\times 0{,}0008 =\n0{,}1996\n        \\end{align*}\\]</span></p>\n"
      }
    }
  ],
  "preview": "<p>Vous allez travailler avec un réseau de neurones à une couche cachée,\nconçu pour effectuer une tâche de prédiction simple.</p>\n<p><strong>Descripti ..."
}